From f07968b7ac7b1e9d7acbe33798fc91a1a041a1e6 Mon Sep 17 00:00:00 2001
From: Alexandru Gagniuc <mr.nuke.me@gmail.com>
Date: Sun, 24 Mar 2024 23:13:48 -0500
Subject: [PATCH 19/19] [HACK]: provide cache manipulation heuristics for EDMA

Because its stupid, not because it makes sense.

Signed-off-by: Alexandru Gagniuc <mr.nuke.me@gmail.com>
---
 Makefile                           |   1 +
 hal/dp_ops/edma_dp/edma_v2/cache.S | 135 +++++++++++++++++++++++++++++
 hal/dp_ops/edma_dp/edma_v2/edma.h  |  21 +++++
 3 files changed, 157 insertions(+)
 create mode 100644 hal/dp_ops/edma_dp/edma_v2/cache.S

diff --git a/Makefile b/Makefile
index e0b0600..83c562f 100644
--- a/Makefile
+++ b/Makefile
@@ -59,6 +59,7 @@ qca-nss-dp-objs += nss_dp_vp_main.o \
 		   hal/dp_ops/edma_dp/edma_v2/edma_misc.o \
 		   hal/dp_ops/edma_dp/edma_v2/edma_rx.o \
 		   hal/dp_ops/edma_dp/edma_v2/edma_tx.o \
+		   hal/dp_ops/edma_dp/edma_v2/cache.o \
 		   hal/gmac_ops/qcom/qcom_if.o \
 		   hal/gmac_ops/syn/xgmac/syn_if.o
 
diff --git a/hal/dp_ops/edma_dp/edma_v2/cache.S b/hal/dp_ops/edma_dp/edma_v2/cache.S
new file mode 100644
index 0000000..97d65c2
--- /dev/null
+++ b/hal/dp_ops/edma_dp/edma_v2/cache.S
@@ -0,0 +1,135 @@
+/* SPDX-License-Identifier: GPL-2.0-only */
+/*
+ * Cache maintenance
+ *
+ * Copyright (C) 2001 Deep Blue Solutions Ltd.
+ * Copyright (C) 2012 ARM Ltd.
+ */
+
+#include <linux/errno.h>
+#include <linux/linkage.h>
+#include <linux/init.h>
+#include <asm/assembler.h>
+
+/*
+ * Macro to perform a data cache maintenance for the interval
+ * [kaddr, kaddr + size)
+ * This macro does not do "data synchronization barrier". Caller should
+ * do "dsb" after transaction.
+ *
+ * 	op:		operation passed to dc instruction
+ * 	kaddr:		starting virtual address of the region
+ * 	size:		size of the region
+ * 	Corrupts:	kaddr, size, tmp1, tmp2
+ */
+	.macro dcache_by_line_op_no_dsb op, kaddr, size, tmp1, tmp2
+	raw_dcache_line_size \tmp1, \tmp2
+	add	\size, \kaddr, \size
+	sub	\tmp2, \tmp1, #1
+	bic	\kaddr, \kaddr, \tmp2
+9998:
+	.ifc	\op, cvau
+	__dcache_op_workaround_clean_cache \op, \kaddr
+	.else
+	.ifc	\op, cvac
+	__dcache_op_workaround_clean_cache \op, \kaddr
+	.else
+	.ifc	\op, cvap
+	sys	3, c7, c12, 1, \kaddr	// dc cvap
+	.else
+	.ifc	\op, cvadp
+	sys	3, c7, c13, 1, \kaddr	// dc cvadp
+	.else
+	dc	\op, \kaddr
+	.endif
+	.endif
+	.endif
+	.endif
+	add	\kaddr, \kaddr, \tmp1
+	cmp	\kaddr, \size
+	b.lo	9998b
+	.endm
+
+/*
+ *	__inval_dcache_area(kaddr, size)
+ *
+ * 	Ensure that any D-cache lines for the interval [kaddr, kaddr+size)
+ * 	are invalidated. Any partial lines at the ends of the interval are
+ *	also cleaned to PoC to prevent data loss.
+ *
+ *	- kaddr   - kernel address
+ *	- size    - size in question
+ */
+SYM_FUNC_START(__inval_dcache_area)
+	/* FALLTHROUGH */
+
+/*
+ *	__dma_inv_area(start, size)
+ *	- start   - virtual start address of region
+ *	- size    - size in question
+ */
+SYM_FUNC_START(__dma_inv_area)
+	add	x1, x1, x0
+	raw_dcache_line_size x2, x3
+	sub	x3, x2, #1
+	tst	x1, x3				// end cache line aligned?
+	bic	x1, x1, x3
+	b.eq	1f
+	dc	civac, x1			// clean & invalidate D / U line
+1:	tst	x0, x3				// start cache line aligned?
+	bic	x0, x0, x3
+	b.eq	2f
+	dc	civac, x0			// clean & invalidate D / U line
+	b	3f
+2:	dc	ivac, x0			// invalidate D / U line
+3:	add	x0, x0, x2
+	cmp	x0, x1
+	b.lo	2b
+	dsb	sy
+	ret
+SYM_FUNC_END(__inval_dcache_area)
+SYM_FUNC_END(__dma_inv_area)
+
+/*
+ *	__dma_inv_area_no_dsb(start, size)
+ *
+ *	This macro does not do "data synchronization barrier". Caller should
+ *	do "dsb" after transaction.
+ *
+ *	- start   - virtual start address of region
+ *	- size    - size in question
+ */
+SYM_FUNC_START(__dma_inv_area_no_dsb)
+	add	x1, x1, x0
+	raw_dcache_line_size x2, x3
+	sub	x3, x2, #1
+	tst	x1, x3				// end cache line aligned?
+	bic	x1, x1, x3
+	b.eq	1f
+	dc	civac, x1			// clean & invalidate D / U line
+1:	tst	x0, x3				// start cache line aligned?
+	bic	x0, x0, x3
+	b.eq	2f
+	dc	civac, x0			// clean & invalidate D / U line
+	b	3f
+2:	dc	ivac, x0			// invalidate D / U line
+3:	add	x0, x0, x2
+	cmp	x0, x1
+	b.lo	2b
+	ret
+SYM_FUNC_END(__dma_inv_area_no_dsb)
+
+
+/*
+ *	__dma_clean_area_no_dsb(start, size)
+ *
+ *	This macro does not do "data synchronization barrier". Caller should
+ *	do "dsb" after transaction.
+ *
+ *	- start   - virtual start address of region
+ *	- size    - size in question
+ */
+SYM_FUNC_START(__dma_clean_area_no_dsb)
+	dcache_by_line_op_no_dsb cvac, x0, x1, x2, x3
+	ret
+SYM_FUNC_END(__dma_clean_area_no_dsb)
diff --git a/hal/dp_ops/edma_dp/edma_v2/edma.h b/hal/dp_ops/edma_dp/edma_v2/edma.h
index f677c97..cbb64af 100644
--- a/hal/dp_ops/edma_dp/edma_v2/edma.h
+++ b/hal/dp_ops/edma_dp/edma_v2/edma.h
@@ -346,4 +346,25 @@ static inline void edma_reg_write(uint32_t reg_off, uint32_t val)
 	hal_write_reg(edma_gbl_ctx.reg_base, reg_off, val);
 }
 
+
+
+extern void __dma_inv_area(const void *start, size_t size);
+extern void __dma_inv_area_no_dsb(const void *start, size_t size);
+extern void __dma_clean_area_no_dsb(const void *start, size_t size);
+
+static inline void dmac_inv_range(const void *start, const void *end)
+{
+	__dma_inv_area(start, (void *)(end) - (void *)(start));
+}
+
+static inline void dmac_inv_range_no_dsb(const void *start, const void *end)
+{
+	__dma_inv_area_no_dsb(start, (void *)(end) - (void *)(start));
+}
+
+static inline void dmac_clean_range_no_dsb(const void *start, const void *end)
+{
+	__dma_clean_area_no_dsb(start, (void *)(end) - (void *)(start));
+}
+
 #endif	/* __EDMA_H__ */
-- 
2.40.1

